{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Regression Model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will build a regression model using the deep learning Keras library, and then I will experiment with increasing the number of training epochs and changing number of hidden layers and you will see how changing these parameters impacts the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Build a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Download and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
       "0            1040.0           676.0   28     79.99  \n",
       "1            1055.0           676.0   28     61.89  \n",
       "2             932.0           594.0  270     40.27  \n",
       "3             932.0           594.0  365     41.05  \n",
       "4             978.4           825.5  360     44.30  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data = pd.read_csv('https://cocl.us/concrete_data')\n",
    "concrete_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataset for any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>281.167864</td>\n",
       "      <td>73.895825</td>\n",
       "      <td>54.188350</td>\n",
       "      <td>181.567282</td>\n",
       "      <td>6.204660</td>\n",
       "      <td>972.918932</td>\n",
       "      <td>773.580485</td>\n",
       "      <td>45.662136</td>\n",
       "      <td>35.817961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.506364</td>\n",
       "      <td>86.279342</td>\n",
       "      <td>63.997004</td>\n",
       "      <td>21.354219</td>\n",
       "      <td>5.973841</td>\n",
       "      <td>77.753954</td>\n",
       "      <td>80.175980</td>\n",
       "      <td>63.169912</td>\n",
       "      <td>16.705742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>192.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>730.950000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>272.900000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>779.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>142.950000</td>\n",
       "      <td>118.300000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1029.400000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>46.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>540.000000</td>\n",
       "      <td>359.400000</td>\n",
       "      <td>200.100000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>992.600000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>82.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Cement  Blast Furnace Slag      Fly Ash        Water  \\\n",
       "count  1030.000000         1030.000000  1030.000000  1030.000000   \n",
       "mean    281.167864           73.895825    54.188350   181.567282   \n",
       "std     104.506364           86.279342    63.997004    21.354219   \n",
       "min     102.000000            0.000000     0.000000   121.800000   \n",
       "25%     192.375000            0.000000     0.000000   164.900000   \n",
       "50%     272.900000           22.000000     0.000000   185.000000   \n",
       "75%     350.000000          142.950000   118.300000   192.000000   \n",
       "max     540.000000          359.400000   200.100000   247.000000   \n",
       "\n",
       "       Superplasticizer  Coarse Aggregate  Fine Aggregate          Age  \\\n",
       "count       1030.000000       1030.000000     1030.000000  1030.000000   \n",
       "mean           6.204660        972.918932      773.580485    45.662136   \n",
       "std            5.973841         77.753954       80.175980    63.169912   \n",
       "min            0.000000        801.000000      594.000000     1.000000   \n",
       "25%            0.000000        932.000000      730.950000     7.000000   \n",
       "50%            6.400000        968.000000      779.500000    28.000000   \n",
       "75%           10.200000       1029.400000      824.000000    56.000000   \n",
       "max           32.200000       1145.000000      992.600000   365.000000   \n",
       "\n",
       "          Strength  \n",
       "count  1030.000000  \n",
       "mean     35.817961  \n",
       "std      16.705742  \n",
       "min       2.330000  \n",
       "25%      23.710000  \n",
       "50%      34.445000  \n",
       "75%      46.135000  \n",
       "max      82.600000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cement                0\n",
       "Blast Furnace Slag    0\n",
       "Fly Ash               0\n",
       "Water                 0\n",
       "Superplasticizer      0\n",
       "Coarse Aggregate      0\n",
       "Fine Aggregate        0\n",
       "Age                   0\n",
       "Strength              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is clean and ready to be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data into predictors and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data_columns = concrete_data.columns\n",
    "\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
    "target = concrete_data['Strength'] # Strength column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  \n",
       "0            1040.0           676.0   28  \n",
       "1            1055.0           676.0   28  \n",
       "2             932.0           594.0  270  \n",
       "3             932.0           594.0  365  \n",
       "4             978.4           825.5  360  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    79.99\n",
       "1    61.89\n",
       "2    40.27\n",
       "3    41.05\n",
       "4    44.30\n",
       "Name: Strength, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of predictors\n",
    "n_cols = predictors.shape[1] \n",
    "n_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 18:16:45.747323: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below creates a model that has one hidden layer with 10 neurons and a ReLU activation function. \n",
    "\n",
    "It uses the adam optimizer and the mean squared error as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def regression_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "model_A = regression_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's import train_test_split function from scikit-learn in order to split the data into a training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data by reserving 30% for test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 - 1s - 25ms/step - loss: 10692.6387\n",
      "Epoch 2/50\n",
      "33/33 - 0s - 2ms/step - loss: 4606.8887\n",
      "Epoch 3/50\n",
      "33/33 - 0s - 2ms/step - loss: 3632.6411\n",
      "Epoch 4/50\n",
      "33/33 - 0s - 1ms/step - loss: 2926.3567\n",
      "Epoch 5/50\n",
      "33/33 - 0s - 2ms/step - loss: 2225.8030\n",
      "Epoch 6/50\n",
      "33/33 - 0s - 2ms/step - loss: 1817.5424\n",
      "Epoch 7/50\n",
      "33/33 - 0s - 2ms/step - loss: 1506.6068\n",
      "Epoch 8/50\n",
      "33/33 - 0s - 2ms/step - loss: 1299.9904\n",
      "Epoch 9/50\n",
      "33/33 - 0s - 2ms/step - loss: 1123.5082\n",
      "Epoch 10/50\n",
      "33/33 - 0s - 3ms/step - loss: 949.5906\n",
      "Epoch 11/50\n",
      "33/33 - 0s - 3ms/step - loss: 822.6143\n",
      "Epoch 12/50\n",
      "33/33 - 0s - 2ms/step - loss: 716.5790\n",
      "Epoch 13/50\n",
      "33/33 - 0s - 2ms/step - loss: 605.9073\n",
      "Epoch 14/50\n",
      "33/33 - 0s - 1ms/step - loss: 528.8033\n",
      "Epoch 15/50\n",
      "33/33 - 0s - 1ms/step - loss: 455.9988\n",
      "Epoch 16/50\n",
      "33/33 - 0s - 2ms/step - loss: 391.3700\n",
      "Epoch 17/50\n",
      "33/33 - 0s - 1ms/step - loss: 337.4530\n",
      "Epoch 18/50\n",
      "33/33 - 0s - 2ms/step - loss: 283.5761\n",
      "Epoch 19/50\n",
      "33/33 - 0s - 2ms/step - loss: 247.2848\n",
      "Epoch 20/50\n",
      "33/33 - 0s - 2ms/step - loss: 219.3492\n",
      "Epoch 21/50\n",
      "33/33 - 0s - 2ms/step - loss: 193.3501\n",
      "Epoch 22/50\n",
      "33/33 - 0s - 2ms/step - loss: 172.7324\n",
      "Epoch 23/50\n",
      "33/33 - 0s - 1ms/step - loss: 154.8547\n",
      "Epoch 24/50\n",
      "33/33 - 0s - 1ms/step - loss: 143.6222\n",
      "Epoch 25/50\n",
      "33/33 - 0s - 1ms/step - loss: 133.5900\n",
      "Epoch 26/50\n",
      "33/33 - 0s - 2ms/step - loss: 126.9633\n",
      "Epoch 27/50\n",
      "33/33 - 0s - 2ms/step - loss: 121.2527\n",
      "Epoch 28/50\n",
      "33/33 - 0s - 1ms/step - loss: 122.9561\n",
      "Epoch 29/50\n",
      "33/33 - 0s - 1ms/step - loss: 116.8959\n",
      "Epoch 30/50\n",
      "33/33 - 0s - 2ms/step - loss: 108.5827\n",
      "Epoch 31/50\n",
      "33/33 - 0s - 1ms/step - loss: 106.3947\n",
      "Epoch 32/50\n",
      "33/33 - 0s - 2ms/step - loss: 104.0043\n",
      "Epoch 33/50\n",
      "33/33 - 0s - 2ms/step - loss: 104.3731\n",
      "Epoch 34/50\n",
      "33/33 - 0s - 2ms/step - loss: 102.8373\n",
      "Epoch 35/50\n",
      "33/33 - 0s - 1ms/step - loss: 100.5690\n",
      "Epoch 36/50\n",
      "33/33 - 0s - 2ms/step - loss: 100.3802\n",
      "Epoch 37/50\n",
      "33/33 - 0s - 1ms/step - loss: 96.2328\n",
      "Epoch 38/50\n",
      "33/33 - 0s - 1ms/step - loss: 97.3460\n",
      "Epoch 39/50\n",
      "33/33 - 0s - 2ms/step - loss: 96.8253\n",
      "Epoch 40/50\n",
      "33/33 - 0s - 2ms/step - loss: 95.0541\n",
      "Epoch 41/50\n",
      "33/33 - 0s - 1ms/step - loss: 100.1035\n",
      "Epoch 42/50\n",
      "33/33 - 0s - 1ms/step - loss: 96.1108\n",
      "Epoch 43/50\n",
      "33/33 - 0s - 2ms/step - loss: 90.3980\n",
      "Epoch 44/50\n",
      "33/33 - 0s - 2ms/step - loss: 90.6744\n",
      "Epoch 45/50\n",
      "33/33 - 0s - 2ms/step - loss: 92.1836\n",
      "Epoch 46/50\n",
      "33/33 - 0s - 2ms/step - loss: 91.1522\n",
      "Epoch 47/50\n",
      "33/33 - 0s - 1ms/step - loss: 91.9053\n",
      "Epoch 48/50\n",
      "33/33 - 0s - 1ms/step - loss: 91.0528\n",
      "Epoch 49/50\n",
      "33/33 - 0s - 1ms/step - loss: 87.3542\n",
      "Epoch 50/50\n",
      "33/33 - 0s - 1ms/step - loss: 92.9527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.history.History at 0x14423ae70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model\n",
    "model = regression_model()\n",
    "\n",
    "# fit the model\n",
    "model.fit(predictors, target, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 1: 68.07587432861328\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "MSE 2: 80.77782440185547\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 3: 67.05425262451172\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 4: 75.60987854003906\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step\n",
      "MSE 5: 63.876625061035156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step\n",
      "MSE 6: 65.05268859863281\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step\n",
      "MSE 7: 80.9613037109375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step\n",
      "MSE 8: 61.64013671875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 9: 57.005462646484375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 10: 59.38519287109375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step\n",
      "MSE 11: 49.52511978149414\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 12: 49.041542053222656\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 13: 58.27982711791992\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 14: 60.169456481933594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 15: 55.79655838012695\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 16: 44.78181838989258\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 17: 53.108734130859375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step\n",
      "MSE 18: 50.62438201904297\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step\n",
      "MSE 19: 51.41138458251953\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step\n",
      "MSE 20: 51.05964279174805\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step\n",
      "MSE 21: 45.8404541015625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step\n",
      "MSE 22: 49.148460388183594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 23: 44.66886901855469\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 24: 47.26091384887695\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 25: 52.24003982543945\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step\n",
      "MSE 26: 49.958641052246094\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 27: 51.844017028808594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 28: 45.108333587646484\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 29: 53.30320358276367\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 30: 51.07489776611328\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 31: 53.83518600463867\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step\n",
      "MSE 32: 56.1038818359375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 33: 49.68803024291992\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 34: 49.877376556396484\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 35: 50.911338806152344\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 36: 51.967803955078125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 37: 54.368873596191406\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step\n",
      "MSE 38: 55.03411102294922\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step\n",
      "MSE 39: 50.73625564575195\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 40: 46.93927764892578\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 41: 53.386940002441406\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step\n",
      "MSE 42: 51.17261505126953\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step\n",
      "MSE 43: 48.39469528198242\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 44: 54.46538162231445\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 45: 54.7906379699707\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE 46: 50.825218200683594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step\n",
      "MSE 47: 50.094337463378906\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 48: 53.36821746826172\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 49: 56.64911651611328\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 50: 49.69357681274414\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\n",
      "\n",
      "Mean and standard deviation of 50 mean squared errors without normalized data. \n",
      " Total number of epochs for each training is: 50\n",
      "\n",
      "Mean: 54.62063655055122\n",
      "Standard Deviation: 8.086717984784507\n"
     ]
    }
   ],
   "source": [
    "# list of 50 mean squared errors\n",
    "total_mean_squared_errors = 50\n",
    "epochs = 50\n",
    "mean_squared_errors = []\n",
    "for i in range(0, total_mean_squared_errors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=i)\n",
    "    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=epochs, verbose=0)\n",
    "    MSE = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"MSE \"+str(i+1)+\": \"+str(MSE))\n",
    "    y_pred = model.predict(X_test)\n",
    "    mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "    mean_squared_errors.append(mean_square_error)\n",
    "\n",
    "mean_squared_errors = np.array(mean_squared_errors)\n",
    "mean_mse = np.mean(mean_squared_errors)\n",
    "sd_mse = np.std(mean_squared_errors)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Mean and standard deviation of \" +str(total_mean_squared_errors) + \" mean squared errors without normalized data. \\n Total number of epochs for each training is: \" +str(epochs) + \"\\n\")\n",
    "print(\"Mean: \"+str(mean_mse))\n",
    "print(\"Standard Deviation: \"+str(sd_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>0.862735</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>1.055651</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>3.551340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>5.055221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.790075</td>\n",
       "      <td>0.678079</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.070492</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>4.976069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n",
       "0  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "1  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "2  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "3  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "4 -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate       Age  \n",
       "0          0.862735       -1.217079 -0.279597  \n",
       "1          1.055651       -1.217079 -0.279597  \n",
       "2         -0.526262       -2.239829  3.551340  \n",
       "3         -0.526262       -2.239829  5.055221  \n",
       "4          0.070492        0.647569  4.976069  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "predictors_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "617     9.31\n",
       "430    24.28\n",
       "838    27.68\n",
       "522    44.52\n",
       "298    48.15\n",
       "Name: Strength, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B = regression_model()\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3)\n",
    "\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 - 1s - 77ms/step - loss: 1648.3643 - val_loss: 1485.5043\n",
      "Epoch 2/50\n",
      "16/16 - 0s - 6ms/step - loss: 1627.9567 - val_loss: 1474.5823\n",
      "Epoch 3/50\n",
      "16/16 - 0s - 5ms/step - loss: 1627.2078 - val_loss: 1463.6111\n",
      "Epoch 4/50\n",
      "16/16 - 0s - 5ms/step - loss: 1607.3328 - val_loss: 1452.5490\n",
      "Epoch 5/50\n",
      "16/16 - 0s - 5ms/step - loss: 1605.0841 - val_loss: 1441.7478\n",
      "Epoch 6/50\n",
      "16/16 - 0s - 5ms/step - loss: 1584.6503 - val_loss: 1430.9357\n",
      "Epoch 7/50\n",
      "16/16 - 0s - 5ms/step - loss: 1570.0598 - val_loss: 1419.7831\n",
      "Epoch 8/50\n",
      "16/16 - 0s - 5ms/step - loss: 1563.8608 - val_loss: 1408.6121\n",
      "Epoch 9/50\n",
      "16/16 - 0s - 5ms/step - loss: 1547.4746 - val_loss: 1397.1316\n",
      "Epoch 10/50\n",
      "16/16 - 0s - 5ms/step - loss: 1533.9926 - val_loss: 1385.1844\n",
      "Epoch 11/50\n",
      "16/16 - 0s - 5ms/step - loss: 1521.0903 - val_loss: 1373.3479\n",
      "Epoch 12/50\n",
      "16/16 - 0s - 6ms/step - loss: 1512.4102 - val_loss: 1361.2357\n",
      "Epoch 13/50\n",
      "16/16 - 0s - 6ms/step - loss: 1504.5931 - val_loss: 1348.5870\n",
      "Epoch 14/50\n",
      "16/16 - 0s - 5ms/step - loss: 1487.5259 - val_loss: 1335.6818\n",
      "Epoch 15/50\n",
      "16/16 - 0s - 6ms/step - loss: 1472.9379 - val_loss: 1322.3982\n",
      "Epoch 16/50\n",
      "16/16 - 0s - 6ms/step - loss: 1454.5818 - val_loss: 1308.7936\n",
      "Epoch 17/50\n",
      "16/16 - 0s - 5ms/step - loss: 1447.4506 - val_loss: 1295.1055\n",
      "Epoch 18/50\n",
      "16/16 - 0s - 5ms/step - loss: 1420.9529 - val_loss: 1280.6295\n",
      "Epoch 19/50\n",
      "16/16 - 0s - 5ms/step - loss: 1412.8574 - val_loss: 1266.1017\n",
      "Epoch 20/50\n",
      "16/16 - 0s - 5ms/step - loss: 1390.9879 - val_loss: 1251.1703\n",
      "Epoch 21/50\n",
      "16/16 - 0s - 5ms/step - loss: 1369.3788 - val_loss: 1235.4017\n",
      "Epoch 22/50\n",
      "16/16 - 0s - 6ms/step - loss: 1359.3451 - val_loss: 1219.7805\n",
      "Epoch 23/50\n",
      "16/16 - 0s - 6ms/step - loss: 1337.2582 - val_loss: 1203.7737\n",
      "Epoch 24/50\n",
      "16/16 - 0s - 6ms/step - loss: 1328.1129 - val_loss: 1187.5255\n",
      "Epoch 25/50\n",
      "16/16 - 0s - 5ms/step - loss: 1301.5431 - val_loss: 1170.5212\n",
      "Epoch 26/50\n",
      "16/16 - 0s - 5ms/step - loss: 1290.6962 - val_loss: 1153.8097\n",
      "Epoch 27/50\n",
      "16/16 - 0s - 5ms/step - loss: 1266.5071 - val_loss: 1136.3066\n",
      "Epoch 28/50\n",
      "16/16 - 0s - 7ms/step - loss: 1241.0010 - val_loss: 1118.2194\n",
      "Epoch 29/50\n",
      "16/16 - 0s - 5ms/step - loss: 1227.7207 - val_loss: 1100.6361\n",
      "Epoch 30/50\n",
      "16/16 - 0s - 5ms/step - loss: 1205.9731 - val_loss: 1082.2869\n",
      "Epoch 31/50\n",
      "16/16 - 0s - 6ms/step - loss: 1189.9293 - val_loss: 1063.9202\n",
      "Epoch 32/50\n",
      "16/16 - 0s - 6ms/step - loss: 1160.2567 - val_loss: 1044.6534\n",
      "Epoch 33/50\n",
      "16/16 - 0s - 8ms/step - loss: 1144.2603 - val_loss: 1025.5848\n",
      "Epoch 34/50\n",
      "16/16 - 0s - 9ms/step - loss: 1123.8896 - val_loss: 1006.6819\n",
      "Epoch 35/50\n",
      "16/16 - 0s - 11ms/step - loss: 1099.2950 - val_loss: 987.0402\n",
      "Epoch 36/50\n",
      "16/16 - 0s - 5ms/step - loss: 1083.7323 - val_loss: 967.3190\n",
      "Epoch 37/50\n",
      "16/16 - 0s - 6ms/step - loss: 1052.7670 - val_loss: 947.1552\n",
      "Epoch 38/50\n",
      "16/16 - 0s - 10ms/step - loss: 1036.7734 - val_loss: 927.5411\n",
      "Epoch 39/50\n",
      "16/16 - 0s - 6ms/step - loss: 1008.7197 - val_loss: 907.3735\n",
      "Epoch 40/50\n",
      "16/16 - 0s - 6ms/step - loss: 994.3008 - val_loss: 887.5273\n",
      "Epoch 41/50\n",
      "16/16 - 0s - 4ms/step - loss: 967.3947 - val_loss: 867.4405\n",
      "Epoch 42/50\n",
      "16/16 - 0s - 4ms/step - loss: 948.7043 - val_loss: 847.3348\n",
      "Epoch 43/50\n",
      "16/16 - 0s - 6ms/step - loss: 918.0786 - val_loss: 826.7056\n",
      "Epoch 44/50\n",
      "16/16 - 0s - 7ms/step - loss: 900.5884 - val_loss: 807.1011\n",
      "Epoch 45/50\n",
      "16/16 - 0s - 9ms/step - loss: 876.3663 - val_loss: 786.8185\n",
      "Epoch 46/50\n",
      "16/16 - 0s - 7ms/step - loss: 855.1160 - val_loss: 766.6957\n",
      "Epoch 47/50\n",
      "16/16 - 0s - 23ms/step - loss: 834.1022 - val_loss: 747.0939\n",
      "Epoch 48/50\n",
      "16/16 - 0s - 8ms/step - loss: 810.5104 - val_loss: 727.1928\n",
      "Epoch 49/50\n",
      "16/16 - 0s - 9ms/step - loss: 787.7468 - val_loss: 707.3174\n",
      "Epoch 50/50\n",
      "16/16 - 0s - 6ms/step - loss: 769.9308 - val_loss: 688.4780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.history.History at 0x1484f8f20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.fit(X_train, y_train, validation_split=0.3, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "753.5317983566122"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_B.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 1: 551.4764404296875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "MSE 2: 198.50271606445312\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step\n",
      "MSE 3: 127.0503158569336\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 4: 103.4461669921875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 5: 82.29214477539062\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step\n",
      "MSE 6: 73.56889343261719\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 7: 67.9311752319336\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 8: 43.35761260986328\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 9: 48.411373138427734\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 10: 44.85700988769531\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 11: 39.766197204589844\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 12: 36.84375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 13: 44.72970199584961\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 14: 44.741886138916016\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 15: 34.20431137084961\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 16: 30.32904052734375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step\n",
      "MSE 17: 33.74502944946289\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 18: 32.785484313964844\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 19: 33.18048858642578\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 20: 32.491249084472656\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 21: 31.66231346130371\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step\n",
      "MSE 22: 32.804222106933594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 23: 28.919769287109375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 24: 28.54311180114746\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 25: 32.557987213134766\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 26: 33.496009826660156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 27: 26.94683837890625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 28: 28.113445281982422\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 29: 33.598167419433594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 30: 31.546764373779297\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 31: 28.527074813842773\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 32: 27.265792846679688\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 33: 28.539337158203125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 34: 30.054790496826172\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 35: 34.79247283935547\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 36: 35.74580764770508\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step\n",
      "MSE 37: 25.510374069213867\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 38: 33.86227035522461\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "MSE 39: 31.338973999023438\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 40: 26.496978759765625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE 41: 31.428110122680664\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step\n",
      "MSE 42: 27.78348731994629\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 43: 27.7121639251709\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 44: 32.98764419555664\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step\n",
      "MSE 45: 31.257537841796875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step\n",
      "MSE 46: 29.83713150024414\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 47: 30.044891357421875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 48: 30.558340072631836\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step\n",
      "MSE 49: 29.963882446289062\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 50: 29.567495346069336\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\n",
      "\n",
      "Mean and standard deviation of 50 mean squared errors with normalized data. \n",
      " Total number of epochs for each training is: 50\n",
      "\n",
      "Mean: 52.057996145117734\n",
      "Standard Deviation: 76.55609885299444\n"
     ]
    }
   ],
   "source": [
    "# list of 50 mean squared errors with normalized data\n",
    "model_B = regression_model()\n",
    "total_mean_squared_errors = 50\n",
    "epochs = 50\n",
    "mse_list_norm = []\n",
    "for i in range(0, total_mean_squared_errors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)\n",
    "    model_B.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=epochs, verbose=0)\n",
    "    MSE = model_B.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"MSE \"+str(i+1)+\": \"+str(MSE))\n",
    "    y_pred = model_B.predict(X_test)\n",
    "    mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "    mse_list_norm.append(mean_square_error)\n",
    "\n",
    "mse_list_norm = np.array(mse_list_norm)\n",
    "mean_mse_norm = np.mean(mse_list_norm)\n",
    "sd_mse_norm = np.std(mse_list_norm)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Mean and standard deviation of \" +str(total_mean_squared_errors) + \" mean squared errors with normalized data. \\n Total number of epochs for each training is: \" +str(epochs) + \"\\n\")\n",
    "print(\"Mean: \"+str(mean_mse_norm))\n",
    "print(\"Standard Deviation: \"+str(sd_mse_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Increate the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_C = regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36     30.08\n",
       "728    31.74\n",
       "38     42.23\n",
       "111    55.90\n",
       "561    33.08\n",
       "Name: Strength, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3)\n",
    "\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 - 1s - 61ms/step - loss: 1527.8424 - val_loss: 1487.4615\n",
      "Epoch 2/100\n",
      "16/16 - 0s - 4ms/step - loss: 1516.7219 - val_loss: 1475.1865\n",
      "Epoch 3/100\n",
      "16/16 - 0s - 5ms/step - loss: 1504.1189 - val_loss: 1462.8893\n",
      "Epoch 4/100\n",
      "16/16 - 0s - 4ms/step - loss: 1486.0966 - val_loss: 1450.3912\n",
      "Epoch 5/100\n",
      "16/16 - 0s - 4ms/step - loss: 1483.5153 - val_loss: 1437.5349\n",
      "Epoch 6/100\n",
      "16/16 - 0s - 5ms/step - loss: 1464.9099 - val_loss: 1424.2053\n",
      "Epoch 7/100\n",
      "16/16 - 0s - 5ms/step - loss: 1449.6230 - val_loss: 1410.4608\n",
      "Epoch 8/100\n",
      "16/16 - 0s - 5ms/step - loss: 1433.3716 - val_loss: 1396.0966\n",
      "Epoch 9/100\n",
      "16/16 - 0s - 4ms/step - loss: 1423.2904 - val_loss: 1381.8763\n",
      "Epoch 10/100\n",
      "16/16 - 0s - 5ms/step - loss: 1408.0142 - val_loss: 1366.9093\n",
      "Epoch 11/100\n",
      "16/16 - 0s - 5ms/step - loss: 1401.5789 - val_loss: 1351.5293\n",
      "Epoch 12/100\n",
      "16/16 - 0s - 4ms/step - loss: 1373.9978 - val_loss: 1335.6635\n",
      "Epoch 13/100\n",
      "16/16 - 0s - 5ms/step - loss: 1355.4323 - val_loss: 1319.3876\n",
      "Epoch 14/100\n",
      "16/16 - 0s - 5ms/step - loss: 1342.6780 - val_loss: 1302.6456\n",
      "Epoch 15/100\n",
      "16/16 - 0s - 4ms/step - loss: 1317.1301 - val_loss: 1285.6057\n",
      "Epoch 16/100\n",
      "16/16 - 0s - 5ms/step - loss: 1303.6444 - val_loss: 1267.8741\n",
      "Epoch 17/100\n",
      "16/16 - 0s - 5ms/step - loss: 1288.9578 - val_loss: 1249.8585\n",
      "Epoch 18/100\n",
      "16/16 - 0s - 5ms/step - loss: 1272.6652 - val_loss: 1231.5314\n",
      "Epoch 19/100\n",
      "16/16 - 0s - 5ms/step - loss: 1243.8669 - val_loss: 1212.3672\n",
      "Epoch 20/100\n",
      "16/16 - 0s - 5ms/step - loss: 1228.6218 - val_loss: 1193.0536\n",
      "Epoch 21/100\n",
      "16/16 - 0s - 4ms/step - loss: 1207.4023 - val_loss: 1173.2054\n",
      "Epoch 22/100\n",
      "16/16 - 0s - 5ms/step - loss: 1186.3629 - val_loss: 1152.8500\n",
      "Epoch 23/100\n",
      "16/16 - 0s - 5ms/step - loss: 1163.6395 - val_loss: 1132.3445\n",
      "Epoch 24/100\n",
      "16/16 - 0s - 5ms/step - loss: 1141.2666 - val_loss: 1111.7327\n",
      "Epoch 25/100\n",
      "16/16 - 0s - 5ms/step - loss: 1120.8949 - val_loss: 1090.7555\n",
      "Epoch 26/100\n",
      "16/16 - 0s - 5ms/step - loss: 1098.3359 - val_loss: 1068.9987\n",
      "Epoch 27/100\n",
      "16/16 - 0s - 5ms/step - loss: 1077.5701 - val_loss: 1047.4500\n",
      "Epoch 28/100\n",
      "16/16 - 0s - 5ms/step - loss: 1053.6332 - val_loss: 1025.6685\n",
      "Epoch 29/100\n",
      "16/16 - 0s - 5ms/step - loss: 1029.3575 - val_loss: 1003.3958\n",
      "Epoch 30/100\n",
      "16/16 - 0s - 5ms/step - loss: 1010.2604 - val_loss: 981.3311\n",
      "Epoch 31/100\n",
      "16/16 - 0s - 5ms/step - loss: 984.1506 - val_loss: 958.8080\n",
      "Epoch 32/100\n",
      "16/16 - 0s - 5ms/step - loss: 959.4152 - val_loss: 936.3209\n",
      "Epoch 33/100\n",
      "16/16 - 0s - 4ms/step - loss: 935.8205 - val_loss: 914.0661\n",
      "Epoch 34/100\n",
      "16/16 - 0s - 5ms/step - loss: 911.2496 - val_loss: 891.2287\n",
      "Epoch 35/100\n",
      "16/16 - 0s - 7ms/step - loss: 892.4636 - val_loss: 868.8292\n",
      "Epoch 36/100\n",
      "16/16 - 0s - 4ms/step - loss: 864.5314 - val_loss: 846.5778\n",
      "Epoch 37/100\n",
      "16/16 - 0s - 5ms/step - loss: 840.8705 - val_loss: 824.3630\n",
      "Epoch 38/100\n",
      "16/16 - 0s - 4ms/step - loss: 821.8811 - val_loss: 802.5010\n",
      "Epoch 39/100\n",
      "16/16 - 0s - 5ms/step - loss: 800.0068 - val_loss: 780.3762\n",
      "Epoch 40/100\n",
      "16/16 - 0s - 5ms/step - loss: 777.3013 - val_loss: 758.8848\n",
      "Epoch 41/100\n",
      "16/16 - 0s - 5ms/step - loss: 753.0507 - val_loss: 737.2318\n",
      "Epoch 42/100\n",
      "16/16 - 0s - 5ms/step - loss: 736.0789 - val_loss: 716.6716\n",
      "Epoch 43/100\n",
      "16/16 - 0s - 4ms/step - loss: 709.6990 - val_loss: 695.8729\n",
      "Epoch 44/100\n",
      "16/16 - 0s - 5ms/step - loss: 685.5750 - val_loss: 675.4067\n",
      "Epoch 45/100\n",
      "16/16 - 0s - 4ms/step - loss: 669.6849 - val_loss: 655.3041\n",
      "Epoch 46/100\n",
      "16/16 - 0s - 5ms/step - loss: 644.8937 - val_loss: 635.6938\n",
      "Epoch 47/100\n",
      "16/16 - 0s - 4ms/step - loss: 624.6245 - val_loss: 616.6868\n",
      "Epoch 48/100\n",
      "16/16 - 0s - 5ms/step - loss: 608.3155 - val_loss: 597.4391\n",
      "Epoch 49/100\n",
      "16/16 - 0s - 4ms/step - loss: 588.3025 - val_loss: 578.7454\n",
      "Epoch 50/100\n",
      "16/16 - 0s - 5ms/step - loss: 566.5297 - val_loss: 560.8402\n",
      "Epoch 51/100\n",
      "16/16 - 0s - 4ms/step - loss: 553.2493 - val_loss: 543.3356\n",
      "Epoch 52/100\n",
      "16/16 - 0s - 5ms/step - loss: 530.5881 - val_loss: 526.1676\n",
      "Epoch 53/100\n",
      "16/16 - 0s - 4ms/step - loss: 514.2462 - val_loss: 510.0561\n",
      "Epoch 54/100\n",
      "16/16 - 0s - 5ms/step - loss: 497.3376 - val_loss: 494.3490\n",
      "Epoch 55/100\n",
      "16/16 - 0s - 4ms/step - loss: 482.2419 - val_loss: 479.0549\n",
      "Epoch 56/100\n",
      "16/16 - 0s - 5ms/step - loss: 467.6953 - val_loss: 464.2134\n",
      "Epoch 57/100\n",
      "16/16 - 0s - 4ms/step - loss: 457.0942 - val_loss: 450.4823\n",
      "Epoch 58/100\n",
      "16/16 - 0s - 5ms/step - loss: 439.5203 - val_loss: 436.5873\n",
      "Epoch 59/100\n",
      "16/16 - 0s - 4ms/step - loss: 423.8571 - val_loss: 422.9099\n",
      "Epoch 60/100\n",
      "16/16 - 0s - 5ms/step - loss: 410.4875 - val_loss: 410.1136\n",
      "Epoch 61/100\n",
      "16/16 - 0s - 5ms/step - loss: 398.7718 - val_loss: 397.8581\n",
      "Epoch 62/100\n",
      "16/16 - 0s - 4ms/step - loss: 386.7976 - val_loss: 386.1956\n",
      "Epoch 63/100\n",
      "16/16 - 0s - 5ms/step - loss: 376.2384 - val_loss: 375.2417\n",
      "Epoch 64/100\n",
      "16/16 - 0s - 5ms/step - loss: 366.8934 - val_loss: 365.0694\n",
      "Epoch 65/100\n",
      "16/16 - 0s - 5ms/step - loss: 358.5439 - val_loss: 354.7696\n",
      "Epoch 66/100\n",
      "16/16 - 0s - 4ms/step - loss: 343.3857 - val_loss: 345.4482\n",
      "Epoch 67/100\n",
      "16/16 - 0s - 4ms/step - loss: 335.1143 - val_loss: 336.4558\n",
      "Epoch 68/100\n",
      "16/16 - 0s - 4ms/step - loss: 325.7576 - val_loss: 328.0143\n",
      "Epoch 69/100\n",
      "16/16 - 0s - 4ms/step - loss: 317.0953 - val_loss: 319.6704\n",
      "Epoch 70/100\n",
      "16/16 - 0s - 5ms/step - loss: 311.0473 - val_loss: 312.4793\n",
      "Epoch 71/100\n",
      "16/16 - 0s - 5ms/step - loss: 304.0719 - val_loss: 305.2630\n",
      "Epoch 72/100\n",
      "16/16 - 0s - 5ms/step - loss: 297.7619 - val_loss: 298.4883\n",
      "Epoch 73/100\n",
      "16/16 - 0s - 4ms/step - loss: 290.9949 - val_loss: 292.2361\n",
      "Epoch 74/100\n",
      "16/16 - 0s - 5ms/step - loss: 282.6987 - val_loss: 286.0519\n",
      "Epoch 75/100\n",
      "16/16 - 0s - 6ms/step - loss: 278.5320 - val_loss: 280.4511\n",
      "Epoch 76/100\n",
      "16/16 - 0s - 5ms/step - loss: 274.3069 - val_loss: 275.0710\n",
      "Epoch 77/100\n",
      "16/16 - 0s - 5ms/step - loss: 268.2706 - val_loss: 269.9724\n",
      "Epoch 78/100\n",
      "16/16 - 0s - 5ms/step - loss: 262.1322 - val_loss: 265.3134\n",
      "Epoch 79/100\n",
      "16/16 - 0s - 5ms/step - loss: 259.0110 - val_loss: 260.8311\n",
      "Epoch 80/100\n",
      "16/16 - 0s - 5ms/step - loss: 254.1282 - val_loss: 256.4640\n",
      "Epoch 81/100\n",
      "16/16 - 0s - 5ms/step - loss: 250.8300 - val_loss: 252.4491\n",
      "Epoch 82/100\n",
      "16/16 - 0s - 5ms/step - loss: 246.0898 - val_loss: 248.5146\n",
      "Epoch 83/100\n",
      "16/16 - 0s - 5ms/step - loss: 244.0659 - val_loss: 244.6718\n",
      "Epoch 84/100\n",
      "16/16 - 0s - 5ms/step - loss: 238.3214 - val_loss: 241.3824\n",
      "Epoch 85/100\n",
      "16/16 - 0s - 5ms/step - loss: 235.2422 - val_loss: 237.9360\n",
      "Epoch 86/100\n",
      "16/16 - 0s - 5ms/step - loss: 232.5944 - val_loss: 234.8420\n",
      "Epoch 87/100\n",
      "16/16 - 0s - 5ms/step - loss: 229.8917 - val_loss: 231.9533\n",
      "Epoch 88/100\n",
      "16/16 - 0s - 5ms/step - loss: 226.9621 - val_loss: 229.0862\n",
      "Epoch 89/100\n",
      "16/16 - 0s - 5ms/step - loss: 223.5192 - val_loss: 226.3225\n",
      "Epoch 90/100\n",
      "16/16 - 0s - 5ms/step - loss: 221.7437 - val_loss: 223.8157\n",
      "Epoch 91/100\n",
      "16/16 - 0s - 6ms/step - loss: 218.1014 - val_loss: 221.3767\n",
      "Epoch 92/100\n",
      "16/16 - 0s - 5ms/step - loss: 216.7848 - val_loss: 218.9626\n",
      "Epoch 93/100\n",
      "16/16 - 0s - 5ms/step - loss: 214.1461 - val_loss: 216.6515\n",
      "Epoch 94/100\n",
      "16/16 - 0s - 5ms/step - loss: 214.1272 - val_loss: 214.3731\n",
      "Epoch 95/100\n",
      "16/16 - 0s - 5ms/step - loss: 211.8352 - val_loss: 212.3127\n",
      "Epoch 96/100\n",
      "16/16 - 0s - 5ms/step - loss: 208.7482 - val_loss: 210.2745\n",
      "Epoch 97/100\n",
      "16/16 - 0s - 5ms/step - loss: 207.8760 - val_loss: 208.2774\n",
      "Epoch 98/100\n",
      "16/16 - 0s - 5ms/step - loss: 206.6190 - val_loss: 206.2782\n",
      "Epoch 99/100\n",
      "16/16 - 0s - 7ms/step - loss: 203.5755 - val_loss: 204.3861\n",
      "Epoch 100/100\n",
      "16/16 - 0s - 5ms/step - loss: 202.8629 - val_loss: 202.5361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.history.History at 0x148a60d70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_C.fit(X_train, y_train, validation_split=0.3, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "186.0069711945003"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_C.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 1: 142.47415161132812\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "MSE 2: 98.06053161621094\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step\n",
      "MSE 3: 60.472084045410156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step\n",
      "MSE 4: 53.28759765625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step\n",
      "MSE 5: 45.91236114501953\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 6: 49.22289276123047\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 7: 50.23561096191406\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 8: 38.36103820800781\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 9: 39.57727813720703\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 10: 41.02722930908203\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step\n",
      "MSE 11: 40.11772918701172\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step\n",
      "MSE 12: 37.204925537109375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step\n",
      "MSE 13: 45.62018585205078\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step\n",
      "MSE 14: 43.09078598022461\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 15: 40.354034423828125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 16: 34.06377410888672\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 17: 37.221351623535156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step\n",
      "MSE 18: 35.074302673339844\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 19: 36.2984504699707\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step\n",
      "MSE 20: 36.629920959472656\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 21: 36.62709045410156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step\n",
      "MSE 22: 34.018218994140625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step\n",
      "MSE 23: 33.1744499206543\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 24: 36.90134811401367\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step\n",
      "MSE 25: 37.088951110839844\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 26: 40.02384567260742\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 27: 35.02560043334961\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 28: 34.290931701660156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 29: 37.95226287841797\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step\n",
      "MSE 30: 37.83889389038086\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 31: 34.718421936035156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 32: 31.69968605041504\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step\n",
      "MSE 33: 34.2310676574707\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 34: 36.50575637817383\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 35: 37.44742202758789\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 36: 41.06132507324219\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step\n",
      "MSE 37: 31.529022216796875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 38: 38.93022537231445\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 39: 35.47404479980469\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 40: 30.5610408782959\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 41: 38.126670837402344\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE 42: 33.394981384277344\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step\n",
      "MSE 43: 34.312416076660156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 44: 37.103431701660156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 45: 38.499908447265625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step\n",
      "MSE 46: 39.42644500732422\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 47: 34.26213836669922\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step\n",
      "MSE 48: 36.74161148071289\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step\n",
      "MSE 49: 34.73976516723633\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 50: 35.96895217895508\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 51: 29.688684463500977\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 52: 31.530813217163086\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 53: 33.59502410888672\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 54: 33.257972717285156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step\n",
      "MSE 55: 38.964088439941406\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 56: 34.5222282409668\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step\n",
      "MSE 57: 31.589569091796875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 58: 32.85112762451172\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 59: 36.62920379638672\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 60: 33.9960823059082\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 61: 34.679054260253906\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 62: 30.444934844970703\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 63: 32.3056526184082\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step\n",
      "MSE 64: 29.398794174194336\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step\n",
      "MSE 65: 33.308265686035156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 66: 33.871360778808594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step\n",
      "MSE 67: 34.18017578125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step\n",
      "MSE 68: 35.16215133666992\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step\n",
      "MSE 69: 30.9151611328125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 70: 31.372732162475586\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step\n",
      "MSE 71: 32.398860931396484\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 72: 29.877676010131836\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 73: 29.073619842529297\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step\n",
      "MSE 74: 32.798763275146484\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step\n",
      "MSE 75: 35.003353118896484\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 76: 31.043609619140625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 77: 29.114776611328125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 78: 29.325164794921875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 79: 36.42584991455078\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 80: 33.961490631103516\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 81: 30.04485511779785\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step\n",
      "MSE 82: 33.2448616027832\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 83: 35.281158447265625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 84: 32.13528060913086\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 85: 29.3211669921875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 86: 30.03163719177246\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 87: 30.20608139038086\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 88: 28.572546005249023\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 89: 27.22394371032715\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE 90: 28.082168579101562\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 91: 32.4835205078125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 92: 26.71076011657715\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 93: 33.260162353515625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 94: 31.293365478515625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 95: 31.014759063720703\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step\n",
      "MSE 96: 31.075702667236328\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 97: 34.05061340332031\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step\n",
      "MSE 98: 30.425928115844727\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 99: 32.9537239074707\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 100: 30.828271865844727\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\n",
      "\n",
      "Mean and standard deviation of 100 mean squared errors with normalized data. \n",
      " Total number of epochs for each training is: 100\n",
      "\n",
      "Mean: 36.81601981643332\n",
      "Standard Deviation: 13.48632291937412\n"
     ]
    }
   ],
   "source": [
    "# list of 100 mean squared errors with normalized data\n",
    "model_C = regression_model()\n",
    "total_mean_squared_errors = 100\n",
    "epochs = 100\n",
    "mse_list_norm_100 = []\n",
    "\n",
    "for i in range(0, total_mean_squared_errors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)\n",
    "    model_C.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=epochs, verbose=0)\n",
    "    MSE = model_C.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"MSE \"+str(i+1)+\": \"+str(MSE))\n",
    "    y_pred = model_C.predict(X_test)\n",
    "    mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "    mse_list_norm_100.append(mean_square_error)\n",
    "\n",
    "mse_list_norm_100 = np.array(mse_list_norm_100)\n",
    "mean_mse_norm_100 = np.mean(mse_list_norm_100)\n",
    "sd_mse_norm_100 = np.std(mse_list_norm_100)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Mean and standard deviation of \" +str(total_mean_squared_errors) + \" mean squared errors with normalized data. \\n Total number of epochs for each training is: \" +str(epochs) + \"\\n\")\n",
    "print(\"Mean: \"+str(mean_mse_norm_100))\n",
    "print(\"Standard Deviation: \"+str(sd_mse_norm_100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

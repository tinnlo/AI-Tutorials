{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Regression Model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will build a regression model using the deep learning Keras library, and then I will experiment with increasing the number of training epochs and changing number of hidden layers and you will see how changing these parameters impacts the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Build a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Download and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
       "0            1040.0           676.0   28     79.99  \n",
       "1            1055.0           676.0   28     61.89  \n",
       "2             932.0           594.0  270     40.27  \n",
       "3             932.0           594.0  365     41.05  \n",
       "4             978.4           825.5  360     44.30  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data = pd.read_csv('https://cocl.us/concrete_data')\n",
    "concrete_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataset for any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>281.167864</td>\n",
       "      <td>73.895825</td>\n",
       "      <td>54.188350</td>\n",
       "      <td>181.567282</td>\n",
       "      <td>6.204660</td>\n",
       "      <td>972.918932</td>\n",
       "      <td>773.580485</td>\n",
       "      <td>45.662136</td>\n",
       "      <td>35.817961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.506364</td>\n",
       "      <td>86.279342</td>\n",
       "      <td>63.997004</td>\n",
       "      <td>21.354219</td>\n",
       "      <td>5.973841</td>\n",
       "      <td>77.753954</td>\n",
       "      <td>80.175980</td>\n",
       "      <td>63.169912</td>\n",
       "      <td>16.705742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>192.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>730.950000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>272.900000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>779.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>142.950000</td>\n",
       "      <td>118.300000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1029.400000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>46.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>540.000000</td>\n",
       "      <td>359.400000</td>\n",
       "      <td>200.100000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>992.600000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>82.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Cement  Blast Furnace Slag      Fly Ash        Water  \\\n",
       "count  1030.000000         1030.000000  1030.000000  1030.000000   \n",
       "mean    281.167864           73.895825    54.188350   181.567282   \n",
       "std     104.506364           86.279342    63.997004    21.354219   \n",
       "min     102.000000            0.000000     0.000000   121.800000   \n",
       "25%     192.375000            0.000000     0.000000   164.900000   \n",
       "50%     272.900000           22.000000     0.000000   185.000000   \n",
       "75%     350.000000          142.950000   118.300000   192.000000   \n",
       "max     540.000000          359.400000   200.100000   247.000000   \n",
       "\n",
       "       Superplasticizer  Coarse Aggregate  Fine Aggregate          Age  \\\n",
       "count       1030.000000       1030.000000     1030.000000  1030.000000   \n",
       "mean           6.204660        972.918932      773.580485    45.662136   \n",
       "std            5.973841         77.753954       80.175980    63.169912   \n",
       "min            0.000000        801.000000      594.000000     1.000000   \n",
       "25%            0.000000        932.000000      730.950000     7.000000   \n",
       "50%            6.400000        968.000000      779.500000    28.000000   \n",
       "75%           10.200000       1029.400000      824.000000    56.000000   \n",
       "max           32.200000       1145.000000      992.600000   365.000000   \n",
       "\n",
       "          Strength  \n",
       "count  1030.000000  \n",
       "mean     35.817961  \n",
       "std      16.705742  \n",
       "min       2.330000  \n",
       "25%      23.710000  \n",
       "50%      34.445000  \n",
       "75%      46.135000  \n",
       "max      82.600000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cement                0\n",
       "Blast Furnace Slag    0\n",
       "Fly Ash               0\n",
       "Water                 0\n",
       "Superplasticizer      0\n",
       "Coarse Aggregate      0\n",
       "Fine Aggregate        0\n",
       "Age                   0\n",
       "Strength              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is clean and ready to be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data into predictors and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data_columns = concrete_data.columns\n",
    "\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
    "target = concrete_data['Strength'] # Strength column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  \n",
       "0            1040.0           676.0   28  \n",
       "1            1055.0           676.0   28  \n",
       "2             932.0           594.0  270  \n",
       "3             932.0           594.0  365  \n",
       "4             978.4           825.5  360  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    79.99\n",
       "1    61.89\n",
       "2    40.27\n",
       "3    41.05\n",
       "4    44.30\n",
       "Name: Strength, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of predictors\n",
    "n_cols = predictors.shape[1] \n",
    "n_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 18:16:45.747323: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below creates a model that has one hidden layer with 10 neurons and a ReLU activation function. \n",
    "\n",
    "It uses the adam optimizer and the mean squared error as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def regression_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "model_A = regression_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's import train_test_split function from scikit-learn in order to split the data into a training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data by reserving 30% for test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 - 1s - 25ms/step - loss: 10692.6387\n",
      "Epoch 2/50\n",
      "33/33 - 0s - 2ms/step - loss: 4606.8887\n",
      "Epoch 3/50\n",
      "33/33 - 0s - 2ms/step - loss: 3632.6411\n",
      "Epoch 4/50\n",
      "33/33 - 0s - 1ms/step - loss: 2926.3567\n",
      "Epoch 5/50\n",
      "33/33 - 0s - 2ms/step - loss: 2225.8030\n",
      "Epoch 6/50\n",
      "33/33 - 0s - 2ms/step - loss: 1817.5424\n",
      "Epoch 7/50\n",
      "33/33 - 0s - 2ms/step - loss: 1506.6068\n",
      "Epoch 8/50\n",
      "33/33 - 0s - 2ms/step - loss: 1299.9904\n",
      "Epoch 9/50\n",
      "33/33 - 0s - 2ms/step - loss: 1123.5082\n",
      "Epoch 10/50\n",
      "33/33 - 0s - 3ms/step - loss: 949.5906\n",
      "Epoch 11/50\n",
      "33/33 - 0s - 3ms/step - loss: 822.6143\n",
      "Epoch 12/50\n",
      "33/33 - 0s - 2ms/step - loss: 716.5790\n",
      "Epoch 13/50\n",
      "33/33 - 0s - 2ms/step - loss: 605.9073\n",
      "Epoch 14/50\n",
      "33/33 - 0s - 1ms/step - loss: 528.8033\n",
      "Epoch 15/50\n",
      "33/33 - 0s - 1ms/step - loss: 455.9988\n",
      "Epoch 16/50\n",
      "33/33 - 0s - 2ms/step - loss: 391.3700\n",
      "Epoch 17/50\n",
      "33/33 - 0s - 1ms/step - loss: 337.4530\n",
      "Epoch 18/50\n",
      "33/33 - 0s - 2ms/step - loss: 283.5761\n",
      "Epoch 19/50\n",
      "33/33 - 0s - 2ms/step - loss: 247.2848\n",
      "Epoch 20/50\n",
      "33/33 - 0s - 2ms/step - loss: 219.3492\n",
      "Epoch 21/50\n",
      "33/33 - 0s - 2ms/step - loss: 193.3501\n",
      "Epoch 22/50\n",
      "33/33 - 0s - 2ms/step - loss: 172.7324\n",
      "Epoch 23/50\n",
      "33/33 - 0s - 1ms/step - loss: 154.8547\n",
      "Epoch 24/50\n",
      "33/33 - 0s - 1ms/step - loss: 143.6222\n",
      "Epoch 25/50\n",
      "33/33 - 0s - 1ms/step - loss: 133.5900\n",
      "Epoch 26/50\n",
      "33/33 - 0s - 2ms/step - loss: 126.9633\n",
      "Epoch 27/50\n",
      "33/33 - 0s - 2ms/step - loss: 121.2527\n",
      "Epoch 28/50\n",
      "33/33 - 0s - 1ms/step - loss: 122.9561\n",
      "Epoch 29/50\n",
      "33/33 - 0s - 1ms/step - loss: 116.8959\n",
      "Epoch 30/50\n",
      "33/33 - 0s - 2ms/step - loss: 108.5827\n",
      "Epoch 31/50\n",
      "33/33 - 0s - 1ms/step - loss: 106.3947\n",
      "Epoch 32/50\n",
      "33/33 - 0s - 2ms/step - loss: 104.0043\n",
      "Epoch 33/50\n",
      "33/33 - 0s - 2ms/step - loss: 104.3731\n",
      "Epoch 34/50\n",
      "33/33 - 0s - 2ms/step - loss: 102.8373\n",
      "Epoch 35/50\n",
      "33/33 - 0s - 1ms/step - loss: 100.5690\n",
      "Epoch 36/50\n",
      "33/33 - 0s - 2ms/step - loss: 100.3802\n",
      "Epoch 37/50\n",
      "33/33 - 0s - 1ms/step - loss: 96.2328\n",
      "Epoch 38/50\n",
      "33/33 - 0s - 1ms/step - loss: 97.3460\n",
      "Epoch 39/50\n",
      "33/33 - 0s - 2ms/step - loss: 96.8253\n",
      "Epoch 40/50\n",
      "33/33 - 0s - 2ms/step - loss: 95.0541\n",
      "Epoch 41/50\n",
      "33/33 - 0s - 1ms/step - loss: 100.1035\n",
      "Epoch 42/50\n",
      "33/33 - 0s - 1ms/step - loss: 96.1108\n",
      "Epoch 43/50\n",
      "33/33 - 0s - 2ms/step - loss: 90.3980\n",
      "Epoch 44/50\n",
      "33/33 - 0s - 2ms/step - loss: 90.6744\n",
      "Epoch 45/50\n",
      "33/33 - 0s - 2ms/step - loss: 92.1836\n",
      "Epoch 46/50\n",
      "33/33 - 0s - 2ms/step - loss: 91.1522\n",
      "Epoch 47/50\n",
      "33/33 - 0s - 1ms/step - loss: 91.9053\n",
      "Epoch 48/50\n",
      "33/33 - 0s - 1ms/step - loss: 91.0528\n",
      "Epoch 49/50\n",
      "33/33 - 0s - 1ms/step - loss: 87.3542\n",
      "Epoch 50/50\n",
      "33/33 - 0s - 1ms/step - loss: 92.9527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.history.History at 0x14423ae70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model\n",
    "model = regression_model()\n",
    "\n",
    "# fit the model\n",
    "model.fit(predictors, target, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 1: 68.07587432861328\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "MSE 2: 80.77782440185547\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 3: 67.05425262451172\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 4: 75.60987854003906\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step\n",
      "MSE 5: 63.876625061035156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step\n",
      "MSE 6: 65.05268859863281\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step\n",
      "MSE 7: 80.9613037109375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step\n",
      "MSE 8: 61.64013671875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 9: 57.005462646484375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 10: 59.38519287109375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step\n",
      "MSE 11: 49.52511978149414\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 12: 49.041542053222656\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 13: 58.27982711791992\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 14: 60.169456481933594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 15: 55.79655838012695\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 16: 44.78181838989258\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 17: 53.108734130859375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step\n",
      "MSE 18: 50.62438201904297\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step\n",
      "MSE 19: 51.41138458251953\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step\n",
      "MSE 20: 51.05964279174805\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step\n",
      "MSE 21: 45.8404541015625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step\n",
      "MSE 22: 49.148460388183594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 23: 44.66886901855469\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 24: 47.26091384887695\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 25: 52.24003982543945\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step\n",
      "MSE 26: 49.958641052246094\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 27: 51.844017028808594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 28: 45.108333587646484\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 29: 53.30320358276367\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 30: 51.07489776611328\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 31: 53.83518600463867\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step\n",
      "MSE 32: 56.1038818359375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 33: 49.68803024291992\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 34: 49.877376556396484\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 35: 50.911338806152344\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 36: 51.967803955078125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 37: 54.368873596191406\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step\n",
      "MSE 38: 55.03411102294922\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step\n",
      "MSE 39: 50.73625564575195\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 40: 46.93927764892578\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 41: 53.386940002441406\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step\n",
      "MSE 42: 51.17261505126953\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step\n",
      "MSE 43: 48.39469528198242\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 44: 54.46538162231445\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 45: 54.7906379699707\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE 46: 50.825218200683594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step\n",
      "MSE 47: 50.094337463378906\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 48: 53.36821746826172\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 49: 56.64911651611328\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 50: 49.69357681274414\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\n",
      "\n",
      "Mean and standard deviation of 50 mean squared errors without normalized data. \n",
      " Total number of epochs for each training is: 50\n",
      "\n",
      "Mean: 54.62063655055122\n",
      "Standard Deviation: 8.086717984784507\n"
     ]
    }
   ],
   "source": [
    "# list of 50 mean squared errors\n",
    "total_mean_squared_errors = 50\n",
    "epochs = 50\n",
    "mean_squared_errors = []\n",
    "for i in range(0, total_mean_squared_errors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=i)\n",
    "    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=epochs, verbose=0)\n",
    "    MSE = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"MSE \"+str(i+1)+\": \"+str(MSE))\n",
    "    y_pred = model.predict(X_test)\n",
    "    mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "    mean_squared_errors.append(mean_square_error)\n",
    "\n",
    "mean_squared_errors = np.array(mean_squared_errors)\n",
    "mean_mse = np.mean(mean_squared_errors)\n",
    "sd_mse = np.std(mean_squared_errors)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Mean and standard deviation of \" +str(total_mean_squared_errors) + \" mean squared errors without normalized data. \\n Total number of epochs for each training is: \" +str(epochs) + \"\\n\")\n",
    "print(\"Mean: \"+str(mean_mse))\n",
    "print(\"Standard Deviation: \"+str(sd_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>0.862735</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>1.055651</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>3.551340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>5.055221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.790075</td>\n",
       "      <td>0.678079</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.070492</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>4.976069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n",
       "0  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "1  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "2  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "3  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "4 -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate       Age  \n",
       "0          0.862735       -1.217079 -0.279597  \n",
       "1          1.055651       -1.217079 -0.279597  \n",
       "2         -0.526262       -2.239829  3.551340  \n",
       "3         -0.526262       -2.239829  5.055221  \n",
       "4          0.070492        0.647569  4.976069  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "predictors_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "617     9.31\n",
       "430    24.28\n",
       "838    27.68\n",
       "522    44.52\n",
       "298    48.15\n",
       "Name: Strength, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B = regression_model()\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3)\n",
    "\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 - 1s - 77ms/step - loss: 1648.3643 - val_loss: 1485.5043\n",
      "Epoch 2/50\n",
      "16/16 - 0s - 6ms/step - loss: 1627.9567 - val_loss: 1474.5823\n",
      "Epoch 3/50\n",
      "16/16 - 0s - 5ms/step - loss: 1627.2078 - val_loss: 1463.6111\n",
      "Epoch 4/50\n",
      "16/16 - 0s - 5ms/step - loss: 1607.3328 - val_loss: 1452.5490\n",
      "Epoch 5/50\n",
      "16/16 - 0s - 5ms/step - loss: 1605.0841 - val_loss: 1441.7478\n",
      "Epoch 6/50\n",
      "16/16 - 0s - 5ms/step - loss: 1584.6503 - val_loss: 1430.9357\n",
      "Epoch 7/50\n",
      "16/16 - 0s - 5ms/step - loss: 1570.0598 - val_loss: 1419.7831\n",
      "Epoch 8/50\n",
      "16/16 - 0s - 5ms/step - loss: 1563.8608 - val_loss: 1408.6121\n",
      "Epoch 9/50\n",
      "16/16 - 0s - 5ms/step - loss: 1547.4746 - val_loss: 1397.1316\n",
      "Epoch 10/50\n",
      "16/16 - 0s - 5ms/step - loss: 1533.9926 - val_loss: 1385.1844\n",
      "Epoch 11/50\n",
      "16/16 - 0s - 5ms/step - loss: 1521.0903 - val_loss: 1373.3479\n",
      "Epoch 12/50\n",
      "16/16 - 0s - 6ms/step - loss: 1512.4102 - val_loss: 1361.2357\n",
      "Epoch 13/50\n",
      "16/16 - 0s - 6ms/step - loss: 1504.5931 - val_loss: 1348.5870\n",
      "Epoch 14/50\n",
      "16/16 - 0s - 5ms/step - loss: 1487.5259 - val_loss: 1335.6818\n",
      "Epoch 15/50\n",
      "16/16 - 0s - 6ms/step - loss: 1472.9379 - val_loss: 1322.3982\n",
      "Epoch 16/50\n",
      "16/16 - 0s - 6ms/step - loss: 1454.5818 - val_loss: 1308.7936\n",
      "Epoch 17/50\n",
      "16/16 - 0s - 5ms/step - loss: 1447.4506 - val_loss: 1295.1055\n",
      "Epoch 18/50\n",
      "16/16 - 0s - 5ms/step - loss: 1420.9529 - val_loss: 1280.6295\n",
      "Epoch 19/50\n",
      "16/16 - 0s - 5ms/step - loss: 1412.8574 - val_loss: 1266.1017\n",
      "Epoch 20/50\n",
      "16/16 - 0s - 5ms/step - loss: 1390.9879 - val_loss: 1251.1703\n",
      "Epoch 21/50\n",
      "16/16 - 0s - 5ms/step - loss: 1369.3788 - val_loss: 1235.4017\n",
      "Epoch 22/50\n",
      "16/16 - 0s - 6ms/step - loss: 1359.3451 - val_loss: 1219.7805\n",
      "Epoch 23/50\n",
      "16/16 - 0s - 6ms/step - loss: 1337.2582 - val_loss: 1203.7737\n",
      "Epoch 24/50\n",
      "16/16 - 0s - 6ms/step - loss: 1328.1129 - val_loss: 1187.5255\n",
      "Epoch 25/50\n",
      "16/16 - 0s - 5ms/step - loss: 1301.5431 - val_loss: 1170.5212\n",
      "Epoch 26/50\n",
      "16/16 - 0s - 5ms/step - loss: 1290.6962 - val_loss: 1153.8097\n",
      "Epoch 27/50\n",
      "16/16 - 0s - 5ms/step - loss: 1266.5071 - val_loss: 1136.3066\n",
      "Epoch 28/50\n",
      "16/16 - 0s - 7ms/step - loss: 1241.0010 - val_loss: 1118.2194\n",
      "Epoch 29/50\n",
      "16/16 - 0s - 5ms/step - loss: 1227.7207 - val_loss: 1100.6361\n",
      "Epoch 30/50\n",
      "16/16 - 0s - 5ms/step - loss: 1205.9731 - val_loss: 1082.2869\n",
      "Epoch 31/50\n",
      "16/16 - 0s - 6ms/step - loss: 1189.9293 - val_loss: 1063.9202\n",
      "Epoch 32/50\n",
      "16/16 - 0s - 6ms/step - loss: 1160.2567 - val_loss: 1044.6534\n",
      "Epoch 33/50\n",
      "16/16 - 0s - 8ms/step - loss: 1144.2603 - val_loss: 1025.5848\n",
      "Epoch 34/50\n",
      "16/16 - 0s - 9ms/step - loss: 1123.8896 - val_loss: 1006.6819\n",
      "Epoch 35/50\n",
      "16/16 - 0s - 11ms/step - loss: 1099.2950 - val_loss: 987.0402\n",
      "Epoch 36/50\n",
      "16/16 - 0s - 5ms/step - loss: 1083.7323 - val_loss: 967.3190\n",
      "Epoch 37/50\n",
      "16/16 - 0s - 6ms/step - loss: 1052.7670 - val_loss: 947.1552\n",
      "Epoch 38/50\n",
      "16/16 - 0s - 10ms/step - loss: 1036.7734 - val_loss: 927.5411\n",
      "Epoch 39/50\n",
      "16/16 - 0s - 6ms/step - loss: 1008.7197 - val_loss: 907.3735\n",
      "Epoch 40/50\n",
      "16/16 - 0s - 6ms/step - loss: 994.3008 - val_loss: 887.5273\n",
      "Epoch 41/50\n",
      "16/16 - 0s - 4ms/step - loss: 967.3947 - val_loss: 867.4405\n",
      "Epoch 42/50\n",
      "16/16 - 0s - 4ms/step - loss: 948.7043 - val_loss: 847.3348\n",
      "Epoch 43/50\n",
      "16/16 - 0s - 6ms/step - loss: 918.0786 - val_loss: 826.7056\n",
      "Epoch 44/50\n",
      "16/16 - 0s - 7ms/step - loss: 900.5884 - val_loss: 807.1011\n",
      "Epoch 45/50\n",
      "16/16 - 0s - 9ms/step - loss: 876.3663 - val_loss: 786.8185\n",
      "Epoch 46/50\n",
      "16/16 - 0s - 7ms/step - loss: 855.1160 - val_loss: 766.6957\n",
      "Epoch 47/50\n",
      "16/16 - 0s - 23ms/step - loss: 834.1022 - val_loss: 747.0939\n",
      "Epoch 48/50\n",
      "16/16 - 0s - 8ms/step - loss: 810.5104 - val_loss: 727.1928\n",
      "Epoch 49/50\n",
      "16/16 - 0s - 9ms/step - loss: 787.7468 - val_loss: 707.3174\n",
      "Epoch 50/50\n",
      "16/16 - 0s - 6ms/step - loss: 769.9308 - val_loss: 688.4780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.history.History at 0x1484f8f20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.fit(X_train, y_train, validation_split=0.3, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "753.5317983566122"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_B.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 1: 551.4764404296875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "MSE 2: 198.50271606445312\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step\n",
      "MSE 3: 127.0503158569336\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 4: 103.4461669921875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 5: 82.29214477539062\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step\n",
      "MSE 6: 73.56889343261719\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 7: 67.9311752319336\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 8: 43.35761260986328\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 9: 48.411373138427734\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 10: 44.85700988769531\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 11: 39.766197204589844\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 12: 36.84375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 13: 44.72970199584961\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 14: 44.741886138916016\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 15: 34.20431137084961\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 16: 30.32904052734375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step\n",
      "MSE 17: 33.74502944946289\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 18: 32.785484313964844\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 19: 33.18048858642578\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 20: 32.491249084472656\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 21: 31.66231346130371\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step\n",
      "MSE 22: 32.804222106933594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 23: 28.919769287109375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 24: 28.54311180114746\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 25: 32.557987213134766\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 26: 33.496009826660156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 27: 26.94683837890625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 28: 28.113445281982422\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 29: 33.598167419433594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 30: 31.546764373779297\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 31: 28.527074813842773\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 32: 27.265792846679688\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 33: 28.539337158203125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 34: 30.054790496826172\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 35: 34.79247283935547\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 36: 35.74580764770508\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step\n",
      "MSE 37: 25.510374069213867\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 38: 33.86227035522461\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "MSE 39: 31.338973999023438\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 40: 26.496978759765625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE 41: 31.428110122680664\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step\n",
      "MSE 42: 27.78348731994629\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 43: 27.7121639251709\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 44: 32.98764419555664\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step\n",
      "MSE 45: 31.257537841796875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step\n",
      "MSE 46: 29.83713150024414\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 47: 30.044891357421875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 48: 30.558340072631836\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step\n",
      "MSE 49: 29.963882446289062\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 50: 29.567495346069336\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\n",
      "\n",
      "Mean and standard deviation of 50 mean squared errors with normalized data. \n",
      " Total number of epochs for each training is: 50\n",
      "\n",
      "Mean: 52.057996145117734\n",
      "Standard Deviation: 76.55609885299444\n"
     ]
    }
   ],
   "source": [
    "# list of 50 mean squared errors with normalized data\n",
    "model_B = regression_model()\n",
    "total_mean_squared_errors = 50\n",
    "epochs = 50\n",
    "mse_list_norm = []\n",
    "for i in range(0, total_mean_squared_errors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)\n",
    "    model_B.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=epochs, verbose=0)\n",
    "    MSE = model_B.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"MSE \"+str(i+1)+\": \"+str(MSE))\n",
    "    y_pred = model_B.predict(X_test)\n",
    "    mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "    mse_list_norm.append(mean_square_error)\n",
    "\n",
    "mse_list_norm = np.array(mse_list_norm)\n",
    "mean_mse_norm = np.mean(mse_list_norm)\n",
    "sd_mse_norm = np.std(mse_list_norm)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Mean and standard deviation of \" +str(total_mean_squared_errors) + \" mean squared errors with normalized data. \\n Total number of epochs for each training is: \" +str(epochs) + \"\\n\")\n",
    "print(\"Mean: \"+str(mean_mse_norm))\n",
    "print(\"Standard Deviation: \"+str(sd_mse_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Increate the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_C = regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36     30.08\n",
       "728    31.74\n",
       "38     42.23\n",
       "111    55.90\n",
       "561    33.08\n",
       "Name: Strength, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3)\n",
    "\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 - 1s - 61ms/step - loss: 1527.8424 - val_loss: 1487.4615\n",
      "Epoch 2/100\n",
      "16/16 - 0s - 4ms/step - loss: 1516.7219 - val_loss: 1475.1865\n",
      "Epoch 3/100\n",
      "16/16 - 0s - 5ms/step - loss: 1504.1189 - val_loss: 1462.8893\n",
      "Epoch 4/100\n",
      "16/16 - 0s - 4ms/step - loss: 1486.0966 - val_loss: 1450.3912\n",
      "Epoch 5/100\n",
      "16/16 - 0s - 4ms/step - loss: 1483.5153 - val_loss: 1437.5349\n",
      "Epoch 6/100\n",
      "16/16 - 0s - 5ms/step - loss: 1464.9099 - val_loss: 1424.2053\n",
      "Epoch 7/100\n",
      "16/16 - 0s - 5ms/step - loss: 1449.6230 - val_loss: 1410.4608\n",
      "Epoch 8/100\n",
      "16/16 - 0s - 5ms/step - loss: 1433.3716 - val_loss: 1396.0966\n",
      "Epoch 9/100\n",
      "16/16 - 0s - 4ms/step - loss: 1423.2904 - val_loss: 1381.8763\n",
      "Epoch 10/100\n",
      "16/16 - 0s - 5ms/step - loss: 1408.0142 - val_loss: 1366.9093\n",
      "Epoch 11/100\n",
      "16/16 - 0s - 5ms/step - loss: 1401.5789 - val_loss: 1351.5293\n",
      "Epoch 12/100\n",
      "16/16 - 0s - 4ms/step - loss: 1373.9978 - val_loss: 1335.6635\n",
      "Epoch 13/100\n",
      "16/16 - 0s - 5ms/step - loss: 1355.4323 - val_loss: 1319.3876\n",
      "Epoch 14/100\n",
      "16/16 - 0s - 5ms/step - loss: 1342.6780 - val_loss: 1302.6456\n",
      "Epoch 15/100\n",
      "16/16 - 0s - 4ms/step - loss: 1317.1301 - val_loss: 1285.6057\n",
      "Epoch 16/100\n",
      "16/16 - 0s - 5ms/step - loss: 1303.6444 - val_loss: 1267.8741\n",
      "Epoch 17/100\n",
      "16/16 - 0s - 5ms/step - loss: 1288.9578 - val_loss: 1249.8585\n",
      "Epoch 18/100\n",
      "16/16 - 0s - 5ms/step - loss: 1272.6652 - val_loss: 1231.5314\n",
      "Epoch 19/100\n",
      "16/16 - 0s - 5ms/step - loss: 1243.8669 - val_loss: 1212.3672\n",
      "Epoch 20/100\n",
      "16/16 - 0s - 5ms/step - loss: 1228.6218 - val_loss: 1193.0536\n",
      "Epoch 21/100\n",
      "16/16 - 0s - 4ms/step - loss: 1207.4023 - val_loss: 1173.2054\n",
      "Epoch 22/100\n",
      "16/16 - 0s - 5ms/step - loss: 1186.3629 - val_loss: 1152.8500\n",
      "Epoch 23/100\n",
      "16/16 - 0s - 5ms/step - loss: 1163.6395 - val_loss: 1132.3445\n",
      "Epoch 24/100\n",
      "16/16 - 0s - 5ms/step - loss: 1141.2666 - val_loss: 1111.7327\n",
      "Epoch 25/100\n",
      "16/16 - 0s - 5ms/step - loss: 1120.8949 - val_loss: 1090.7555\n",
      "Epoch 26/100\n",
      "16/16 - 0s - 5ms/step - loss: 1098.3359 - val_loss: 1068.9987\n",
      "Epoch 27/100\n",
      "16/16 - 0s - 5ms/step - loss: 1077.5701 - val_loss: 1047.4500\n",
      "Epoch 28/100\n",
      "16/16 - 0s - 5ms/step - loss: 1053.6332 - val_loss: 1025.6685\n",
      "Epoch 29/100\n",
      "16/16 - 0s - 5ms/step - loss: 1029.3575 - val_loss: 1003.3958\n",
      "Epoch 30/100\n",
      "16/16 - 0s - 5ms/step - loss: 1010.2604 - val_loss: 981.3311\n",
      "Epoch 31/100\n",
      "16/16 - 0s - 5ms/step - loss: 984.1506 - val_loss: 958.8080\n",
      "Epoch 32/100\n",
      "16/16 - 0s - 5ms/step - loss: 959.4152 - val_loss: 936.3209\n",
      "Epoch 33/100\n",
      "16/16 - 0s - 4ms/step - loss: 935.8205 - val_loss: 914.0661\n",
      "Epoch 34/100\n",
      "16/16 - 0s - 5ms/step - loss: 911.2496 - val_loss: 891.2287\n",
      "Epoch 35/100\n",
      "16/16 - 0s - 7ms/step - loss: 892.4636 - val_loss: 868.8292\n",
      "Epoch 36/100\n",
      "16/16 - 0s - 4ms/step - loss: 864.5314 - val_loss: 846.5778\n",
      "Epoch 37/100\n",
      "16/16 - 0s - 5ms/step - loss: 840.8705 - val_loss: 824.3630\n",
      "Epoch 38/100\n",
      "16/16 - 0s - 4ms/step - loss: 821.8811 - val_loss: 802.5010\n",
      "Epoch 39/100\n",
      "16/16 - 0s - 5ms/step - loss: 800.0068 - val_loss: 780.3762\n",
      "Epoch 40/100\n",
      "16/16 - 0s - 5ms/step - loss: 777.3013 - val_loss: 758.8848\n",
      "Epoch 41/100\n",
      "16/16 - 0s - 5ms/step - loss: 753.0507 - val_loss: 737.2318\n",
      "Epoch 42/100\n",
      "16/16 - 0s - 5ms/step - loss: 736.0789 - val_loss: 716.6716\n",
      "Epoch 43/100\n",
      "16/16 - 0s - 4ms/step - loss: 709.6990 - val_loss: 695.8729\n",
      "Epoch 44/100\n",
      "16/16 - 0s - 5ms/step - loss: 685.5750 - val_loss: 675.4067\n",
      "Epoch 45/100\n",
      "16/16 - 0s - 4ms/step - loss: 669.6849 - val_loss: 655.3041\n",
      "Epoch 46/100\n",
      "16/16 - 0s - 5ms/step - loss: 644.8937 - val_loss: 635.6938\n",
      "Epoch 47/100\n",
      "16/16 - 0s - 4ms/step - loss: 624.6245 - val_loss: 616.6868\n",
      "Epoch 48/100\n",
      "16/16 - 0s - 5ms/step - loss: 608.3155 - val_loss: 597.4391\n",
      "Epoch 49/100\n",
      "16/16 - 0s - 4ms/step - loss: 588.3025 - val_loss: 578.7454\n",
      "Epoch 50/100\n",
      "16/16 - 0s - 5ms/step - loss: 566.5297 - val_loss: 560.8402\n",
      "Epoch 51/100\n",
      "16/16 - 0s - 4ms/step - loss: 553.2493 - val_loss: 543.3356\n",
      "Epoch 52/100\n",
      "16/16 - 0s - 5ms/step - loss: 530.5881 - val_loss: 526.1676\n",
      "Epoch 53/100\n",
      "16/16 - 0s - 4ms/step - loss: 514.2462 - val_loss: 510.0561\n",
      "Epoch 54/100\n",
      "16/16 - 0s - 5ms/step - loss: 497.3376 - val_loss: 494.3490\n",
      "Epoch 55/100\n",
      "16/16 - 0s - 4ms/step - loss: 482.2419 - val_loss: 479.0549\n",
      "Epoch 56/100\n",
      "16/16 - 0s - 5ms/step - loss: 467.6953 - val_loss: 464.2134\n",
      "Epoch 57/100\n",
      "16/16 - 0s - 4ms/step - loss: 457.0942 - val_loss: 450.4823\n",
      "Epoch 58/100\n",
      "16/16 - 0s - 5ms/step - loss: 439.5203 - val_loss: 436.5873\n",
      "Epoch 59/100\n",
      "16/16 - 0s - 4ms/step - loss: 423.8571 - val_loss: 422.9099\n",
      "Epoch 60/100\n",
      "16/16 - 0s - 5ms/step - loss: 410.4875 - val_loss: 410.1136\n",
      "Epoch 61/100\n",
      "16/16 - 0s - 5ms/step - loss: 398.7718 - val_loss: 397.8581\n",
      "Epoch 62/100\n",
      "16/16 - 0s - 4ms/step - loss: 386.7976 - val_loss: 386.1956\n",
      "Epoch 63/100\n",
      "16/16 - 0s - 5ms/step - loss: 376.2384 - val_loss: 375.2417\n",
      "Epoch 64/100\n",
      "16/16 - 0s - 5ms/step - loss: 366.8934 - val_loss: 365.0694\n",
      "Epoch 65/100\n",
      "16/16 - 0s - 5ms/step - loss: 358.5439 - val_loss: 354.7696\n",
      "Epoch 66/100\n",
      "16/16 - 0s - 4ms/step - loss: 343.3857 - val_loss: 345.4482\n",
      "Epoch 67/100\n",
      "16/16 - 0s - 4ms/step - loss: 335.1143 - val_loss: 336.4558\n",
      "Epoch 68/100\n",
      "16/16 - 0s - 4ms/step - loss: 325.7576 - val_loss: 328.0143\n",
      "Epoch 69/100\n",
      "16/16 - 0s - 4ms/step - loss: 317.0953 - val_loss: 319.6704\n",
      "Epoch 70/100\n",
      "16/16 - 0s - 5ms/step - loss: 311.0473 - val_loss: 312.4793\n",
      "Epoch 71/100\n",
      "16/16 - 0s - 5ms/step - loss: 304.0719 - val_loss: 305.2630\n",
      "Epoch 72/100\n",
      "16/16 - 0s - 5ms/step - loss: 297.7619 - val_loss: 298.4883\n",
      "Epoch 73/100\n",
      "16/16 - 0s - 4ms/step - loss: 290.9949 - val_loss: 292.2361\n",
      "Epoch 74/100\n",
      "16/16 - 0s - 5ms/step - loss: 282.6987 - val_loss: 286.0519\n",
      "Epoch 75/100\n",
      "16/16 - 0s - 6ms/step - loss: 278.5320 - val_loss: 280.4511\n",
      "Epoch 76/100\n",
      "16/16 - 0s - 5ms/step - loss: 274.3069 - val_loss: 275.0710\n",
      "Epoch 77/100\n",
      "16/16 - 0s - 5ms/step - loss: 268.2706 - val_loss: 269.9724\n",
      "Epoch 78/100\n",
      "16/16 - 0s - 5ms/step - loss: 262.1322 - val_loss: 265.3134\n",
      "Epoch 79/100\n",
      "16/16 - 0s - 5ms/step - loss: 259.0110 - val_loss: 260.8311\n",
      "Epoch 80/100\n",
      "16/16 - 0s - 5ms/step - loss: 254.1282 - val_loss: 256.4640\n",
      "Epoch 81/100\n",
      "16/16 - 0s - 5ms/step - loss: 250.8300 - val_loss: 252.4491\n",
      "Epoch 82/100\n",
      "16/16 - 0s - 5ms/step - loss: 246.0898 - val_loss: 248.5146\n",
      "Epoch 83/100\n",
      "16/16 - 0s - 5ms/step - loss: 244.0659 - val_loss: 244.6718\n",
      "Epoch 84/100\n",
      "16/16 - 0s - 5ms/step - loss: 238.3214 - val_loss: 241.3824\n",
      "Epoch 85/100\n",
      "16/16 - 0s - 5ms/step - loss: 235.2422 - val_loss: 237.9360\n",
      "Epoch 86/100\n",
      "16/16 - 0s - 5ms/step - loss: 232.5944 - val_loss: 234.8420\n",
      "Epoch 87/100\n",
      "16/16 - 0s - 5ms/step - loss: 229.8917 - val_loss: 231.9533\n",
      "Epoch 88/100\n",
      "16/16 - 0s - 5ms/step - loss: 226.9621 - val_loss: 229.0862\n",
      "Epoch 89/100\n",
      "16/16 - 0s - 5ms/step - loss: 223.5192 - val_loss: 226.3225\n",
      "Epoch 90/100\n",
      "16/16 - 0s - 5ms/step - loss: 221.7437 - val_loss: 223.8157\n",
      "Epoch 91/100\n",
      "16/16 - 0s - 6ms/step - loss: 218.1014 - val_loss: 221.3767\n",
      "Epoch 92/100\n",
      "16/16 - 0s - 5ms/step - loss: 216.7848 - val_loss: 218.9626\n",
      "Epoch 93/100\n",
      "16/16 - 0s - 5ms/step - loss: 214.1461 - val_loss: 216.6515\n",
      "Epoch 94/100\n",
      "16/16 - 0s - 5ms/step - loss: 214.1272 - val_loss: 214.3731\n",
      "Epoch 95/100\n",
      "16/16 - 0s - 5ms/step - loss: 211.8352 - val_loss: 212.3127\n",
      "Epoch 96/100\n",
      "16/16 - 0s - 5ms/step - loss: 208.7482 - val_loss: 210.2745\n",
      "Epoch 97/100\n",
      "16/16 - 0s - 5ms/step - loss: 207.8760 - val_loss: 208.2774\n",
      "Epoch 98/100\n",
      "16/16 - 0s - 5ms/step - loss: 206.6190 - val_loss: 206.2782\n",
      "Epoch 99/100\n",
      "16/16 - 0s - 7ms/step - loss: 203.5755 - val_loss: 204.3861\n",
      "Epoch 100/100\n",
      "16/16 - 0s - 5ms/step - loss: 202.8629 - val_loss: 202.5361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.history.History at 0x148a60d70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_C.fit(X_train, y_train, validation_split=0.3, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "186.0069711945003"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_C.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 1: 142.47415161132812\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "MSE 2: 98.06053161621094\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step\n",
      "MSE 3: 60.472084045410156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step\n",
      "MSE 4: 53.28759765625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step\n",
      "MSE 5: 45.91236114501953\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 6: 49.22289276123047\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 7: 50.23561096191406\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 8: 38.36103820800781\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 9: 39.57727813720703\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 10: 41.02722930908203\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step\n",
      "MSE 11: 40.11772918701172\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step\n",
      "MSE 12: 37.204925537109375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step\n",
      "MSE 13: 45.62018585205078\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step\n",
      "MSE 14: 43.09078598022461\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 15: 40.354034423828125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 16: 34.06377410888672\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 17: 37.221351623535156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step\n",
      "MSE 18: 35.074302673339844\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 19: 36.2984504699707\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step\n",
      "MSE 20: 36.629920959472656\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 21: 36.62709045410156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step\n",
      "MSE 22: 34.018218994140625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step\n",
      "MSE 23: 33.1744499206543\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 24: 36.90134811401367\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step\n",
      "MSE 25: 37.088951110839844\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 26: 40.02384567260742\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 27: 35.02560043334961\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 28: 34.290931701660156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 29: 37.95226287841797\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step\n",
      "MSE 30: 37.83889389038086\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 31: 34.718421936035156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 32: 31.69968605041504\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step\n",
      "MSE 33: 34.2310676574707\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 34: 36.50575637817383\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 35: 37.44742202758789\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 36: 41.06132507324219\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step\n",
      "MSE 37: 31.529022216796875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 38: 38.93022537231445\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 39: 35.47404479980469\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 40: 30.5610408782959\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 41: 38.126670837402344\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE 42: 33.394981384277344\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step\n",
      "MSE 43: 34.312416076660156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 44: 37.103431701660156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 45: 38.499908447265625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step\n",
      "MSE 46: 39.42644500732422\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 47: 34.26213836669922\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step\n",
      "MSE 48: 36.74161148071289\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step\n",
      "MSE 49: 34.73976516723633\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 50: 35.96895217895508\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 51: 29.688684463500977\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 52: 31.530813217163086\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 53: 33.59502410888672\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 54: 33.257972717285156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step\n",
      "MSE 55: 38.964088439941406\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 56: 34.5222282409668\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step\n",
      "MSE 57: 31.589569091796875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 58: 32.85112762451172\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 59: 36.62920379638672\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 60: 33.9960823059082\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 61: 34.679054260253906\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 62: 30.444934844970703\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 63: 32.3056526184082\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step\n",
      "MSE 64: 29.398794174194336\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step\n",
      "MSE 65: 33.308265686035156\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 66: 33.871360778808594\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step\n",
      "MSE 67: 34.18017578125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step\n",
      "MSE 68: 35.16215133666992\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step\n",
      "MSE 69: 30.9151611328125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 70: 31.372732162475586\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step\n",
      "MSE 71: 32.398860931396484\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 72: 29.877676010131836\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 73: 29.073619842529297\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step\n",
      "MSE 74: 32.798763275146484\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step\n",
      "MSE 75: 35.003353118896484\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 76: 31.043609619140625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 77: 29.114776611328125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 78: 29.325164794921875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 79: 36.42584991455078\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 80: 33.961490631103516\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 81: 30.04485511779785\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step\n",
      "MSE 82: 33.2448616027832\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 83: 35.281158447265625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 84: 32.13528060913086\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 85: 29.3211669921875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 86: 30.03163719177246\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 87: 30.20608139038086\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 88: 28.572546005249023\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 89: 27.22394371032715\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE 90: 28.082168579101562\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 91: 32.4835205078125\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 92: 26.71076011657715\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 93: 33.260162353515625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 94: 31.293365478515625\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 95: 31.014759063720703\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step\n",
      "MSE 96: 31.075702667236328\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 97: 34.05061340332031\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step\n",
      "MSE 98: 30.425928115844727\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 99: 32.9537239074707\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 100: 30.828271865844727\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\n",
      "\n",
      "Mean and standard deviation of 100 mean squared errors with normalized data. \n",
      " Total number of epochs for each training is: 100\n",
      "\n",
      "Mean: 36.81601981643332\n",
      "Standard Deviation: 13.48632291937412\n"
     ]
    }
   ],
   "source": [
    "# list of 100 mean squared errors with normalized data\n",
    "model_C = regression_model()\n",
    "total_mean_squared_errors = 100\n",
    "epochs = 100\n",
    "mse_list_norm_100 = []\n",
    "\n",
    "for i in range(0, total_mean_squared_errors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)\n",
    "    model_C.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=epochs, verbose=0)\n",
    "    MSE = model_C.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"MSE \"+str(i+1)+\": \"+str(MSE))\n",
    "    y_pred = model_C.predict(X_test)\n",
    "    mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "    mse_list_norm_100.append(mean_square_error)\n",
    "\n",
    "mse_list_norm_100 = np.array(mse_list_norm_100)\n",
    "mean_mse_norm_100 = np.mean(mse_list_norm_100)\n",
    "sd_mse_norm_100 = np.std(mse_list_norm_100)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Mean and standard deviation of \" +str(total_mean_squared_errors) + \" mean squared errors with normalized data. \\n Total number of epochs for each training is: \" +str(epochs) + \"\\n\")\n",
    "print(\"Mean: \"+str(mean_mse_norm_100))\n",
    "print(\"Standard Deviation: \"+str(sd_mse_norm_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Increase the number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def regression_model_D():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "model_D = regression_model_D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "989    19.99\n",
       "448    51.72\n",
       "396    41.37\n",
       "471    57.03\n",
       "742    54.28\n",
       "Name: Strength, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3)\n",
    "\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 - 2s - 98ms/step - loss: 1541.9049 - val_loss: 1466.2676\n",
      "Epoch 2/50\n",
      "16/16 - 0s - 6ms/step - loss: 1512.9310 - val_loss: 1443.1031\n",
      "Epoch 3/50\n",
      "16/16 - 0s - 5ms/step - loss: 1485.6206 - val_loss: 1414.3662\n",
      "Epoch 4/50\n",
      "16/16 - 0s - 5ms/step - loss: 1455.1257 - val_loss: 1377.2780\n",
      "Epoch 5/50\n",
      "16/16 - 0s - 5ms/step - loss: 1409.9385 - val_loss: 1327.9036\n",
      "Epoch 6/50\n",
      "16/16 - 0s - 5ms/step - loss: 1359.6826 - val_loss: 1263.1024\n",
      "Epoch 7/50\n",
      "16/16 - 0s - 5ms/step - loss: 1273.0798 - val_loss: 1181.7490\n",
      "Epoch 8/50\n",
      "16/16 - 0s - 5ms/step - loss: 1176.5270 - val_loss: 1083.0708\n",
      "Epoch 9/50\n",
      "16/16 - 0s - 5ms/step - loss: 1060.5013 - val_loss: 968.3497\n",
      "Epoch 10/50\n",
      "16/16 - 0s - 6ms/step - loss: 936.9034 - val_loss: 837.9088\n",
      "Epoch 11/50\n",
      "16/16 - 0s - 5ms/step - loss: 790.3918 - val_loss: 698.9240\n",
      "Epoch 12/50\n",
      "16/16 - 0s - 5ms/step - loss: 639.5156 - val_loss: 565.7397\n",
      "Epoch 13/50\n",
      "16/16 - 0s - 5ms/step - loss: 512.8250 - val_loss: 454.3957\n",
      "Epoch 14/50\n",
      "16/16 - 0s - 5ms/step - loss: 410.2923 - val_loss: 376.6472\n",
      "Epoch 15/50\n",
      "16/16 - 0s - 5ms/step - loss: 341.3920 - val_loss: 327.4424\n",
      "Epoch 16/50\n",
      "16/16 - 0s - 5ms/step - loss: 304.0044 - val_loss: 300.6869\n",
      "Epoch 17/50\n",
      "16/16 - 0s - 5ms/step - loss: 282.7387 - val_loss: 281.6271\n",
      "Epoch 18/50\n",
      "16/16 - 0s - 5ms/step - loss: 269.1630 - val_loss: 265.7780\n",
      "Epoch 19/50\n",
      "16/16 - 0s - 5ms/step - loss: 254.0227 - val_loss: 252.7271\n",
      "Epoch 20/50\n",
      "16/16 - 0s - 5ms/step - loss: 241.6123 - val_loss: 241.0446\n",
      "Epoch 21/50\n",
      "16/16 - 0s - 9ms/step - loss: 231.4745 - val_loss: 230.3541\n",
      "Epoch 22/50\n",
      "16/16 - 0s - 5ms/step - loss: 221.5605 - val_loss: 220.8336\n",
      "Epoch 23/50\n",
      "16/16 - 0s - 5ms/step - loss: 214.2463 - val_loss: 211.5790\n",
      "Epoch 24/50\n",
      "16/16 - 0s - 5ms/step - loss: 204.8348 - val_loss: 202.9313\n",
      "Epoch 25/50\n",
      "16/16 - 0s - 5ms/step - loss: 194.4448 - val_loss: 194.5836\n",
      "Epoch 26/50\n",
      "16/16 - 0s - 6ms/step - loss: 187.9542 - val_loss: 187.1102\n",
      "Epoch 27/50\n",
      "16/16 - 0s - 5ms/step - loss: 181.2296 - val_loss: 179.7316\n",
      "Epoch 28/50\n",
      "16/16 - 0s - 5ms/step - loss: 173.6180 - val_loss: 173.4414\n",
      "Epoch 29/50\n",
      "16/16 - 0s - 5ms/step - loss: 169.1457 - val_loss: 167.4869\n",
      "Epoch 30/50\n",
      "16/16 - 0s - 5ms/step - loss: 164.1892 - val_loss: 161.4307\n",
      "Epoch 31/50\n",
      "16/16 - 0s - 5ms/step - loss: 157.9338 - val_loss: 156.6463\n",
      "Epoch 32/50\n",
      "16/16 - 0s - 5ms/step - loss: 153.3401 - val_loss: 151.9327\n",
      "Epoch 33/50\n",
      "16/16 - 0s - 5ms/step - loss: 147.8700 - val_loss: 147.6111\n",
      "Epoch 34/50\n",
      "16/16 - 0s - 5ms/step - loss: 145.1225 - val_loss: 143.4334\n",
      "Epoch 35/50\n",
      "16/16 - 0s - 5ms/step - loss: 141.1275 - val_loss: 139.5388\n",
      "Epoch 36/50\n",
      "16/16 - 0s - 5ms/step - loss: 136.6412 - val_loss: 135.9651\n",
      "Epoch 37/50\n",
      "16/16 - 0s - 5ms/step - loss: 132.9811 - val_loss: 132.2877\n",
      "Epoch 38/50\n",
      "16/16 - 0s - 5ms/step - loss: 129.5192 - val_loss: 129.1346\n",
      "Epoch 39/50\n",
      "16/16 - 0s - 6ms/step - loss: 126.3325 - val_loss: 126.3234\n",
      "Epoch 40/50\n",
      "16/16 - 0s - 5ms/step - loss: 123.1763 - val_loss: 123.5549\n",
      "Epoch 41/50\n",
      "16/16 - 0s - 5ms/step - loss: 121.4242 - val_loss: 120.7031\n",
      "Epoch 42/50\n",
      "16/16 - 0s - 5ms/step - loss: 118.8102 - val_loss: 118.4440\n",
      "Epoch 43/50\n",
      "16/16 - 0s - 5ms/step - loss: 116.4451 - val_loss: 115.9865\n",
      "Epoch 44/50\n",
      "16/16 - 0s - 5ms/step - loss: 113.2739 - val_loss: 114.0571\n",
      "Epoch 45/50\n",
      "16/16 - 0s - 5ms/step - loss: 111.1594 - val_loss: 111.8950\n",
      "Epoch 46/50\n",
      "16/16 - 0s - 5ms/step - loss: 108.6948 - val_loss: 110.0376\n",
      "Epoch 47/50\n",
      "16/16 - 0s - 5ms/step - loss: 108.3806 - val_loss: 108.1436\n",
      "Epoch 48/50\n",
      "16/16 - 0s - 5ms/step - loss: 104.9970 - val_loss: 106.3872\n",
      "Epoch 49/50\n",
      "16/16 - 0s - 6ms/step - loss: 104.1404 - val_loss: 104.8804\n",
      "Epoch 50/50\n",
      "16/16 - 0s - 6ms/step - loss: 102.3855 - val_loss: 103.0792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.history.History at 0x1484f8c80>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_D.fit(X_train, y_train, validation_split=0.3, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102.88729568166188"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_D.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lxt/anaconda3/envs/GenAI/lib/python3.12/site-packages/keras/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 1: 94.68505859375\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "MSE 2: 72.87254333496094\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 3: 45.043785095214844\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 4: 38.618614196777344\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step\n",
      "MSE 5: 35.2330436706543\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 6: 37.67277526855469\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 7: 42.36659240722656\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 8: 34.104549407958984\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 9: 36.145591735839844\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 10: 34.87469482421875\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 11: 29.716238021850586\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step\n",
      "MSE 12: 26.92343521118164\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "MSE 13: 32.02882766723633\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 14: 32.55986785888672\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 15: 30.571269989013672\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 16: 23.184520721435547\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step\n",
      "MSE 17: 27.014419555664062\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step\n",
      "MSE 18: 24.677871704101562\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step\n",
      "MSE 19: 25.710763931274414\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 20: 26.795330047607422\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 21: 25.97051429748535\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 22: 25.236360549926758\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 23: 21.503026962280273\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 24: 24.825220108032227\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 25: 27.081714630126953\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 26: 27.898876190185547\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step\n",
      "MSE 27: 25.77634048461914\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 28: 22.9238224029541\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step\n",
      "MSE 29: 24.341535568237305\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 30: 25.403894424438477\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step\n",
      "MSE 31: 25.817867279052734\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "MSE 32: 21.98129653930664\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 33: 21.839319229125977\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 34: 22.57911491394043\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 35: 24.430471420288086\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 36: 25.647510528564453\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 37: 18.430652618408203\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 38: 24.241016387939453\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 39: 25.12320327758789\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 40: 18.256031036376953\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 41: 23.534513473510742\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "MSE 42: 20.22243881225586\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 43: 18.665706634521484\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 44: 21.14278221130371\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 45: 25.355329513549805\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 46: 19.14894676208496\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 47: 21.062368392944336\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 48: 19.46112823486328\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 49: 20.579561233520508\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "MSE 50: 21.067241668701172\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\n",
      "\n",
      "Mean and standard deviation of 50 mean squared errors with normalized data. \n",
      " Total number of epochs for each training is: 50\n",
      "\n",
      "Mean: 28.696288404419615\n",
      "Standard Deviation: 12.984027119867813\n"
     ]
    }
   ],
   "source": [
    "# list of 50 mean squared errors with normalized data and three hidden layers\n",
    "model_D = regression_model_D()\n",
    "total_mean_squared_errors = 50\n",
    "epochs = 50\n",
    "mse_list_norm_ml = []\n",
    "\n",
    "for i in range(0, total_mean_squared_errors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)\n",
    "    model_D.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=epochs, verbose=0)\n",
    "    MSE = model_D.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"MSE \"+str(i+1)+\": \"+str(MSE))\n",
    "    y_pred = model_D.predict(X_test)\n",
    "    mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "    mse_list_norm_ml.append(mean_square_error)\n",
    "\n",
    "mse_list_norm_ml = np.array(mse_list_norm_ml)\n",
    "mean_mse_norm_ml = np.mean(mse_list_norm_ml)\n",
    "sd_mse_norm_ml = np.std(mse_list_norm_ml)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Mean and standard deviation of \" +str(total_mean_squared_errors) + \" mean squared errors with normalized data. \\n Total number of epochs for each training is: \" +str(epochs) + \"\\n\")\n",
    "print(\"Mean: \"+str(mean_mse_norm_ml))\n",
    "print(\"Standard Deviation: \"+str(sd_mse_norm_ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Mean Squared Error in all the 4 sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A\n",
      "Mean MSE:  54.62063655055122\n",
      "Standard deviation of MSE:  8.086717984784507\n",
      "\n",
      "B\n",
      "Mean MSE (normalized data):  52.057996145117734\n",
      "Standard deviation of MSE (normalized data):  76.55609885299444\n",
      "\n",
      "C\n",
      "Mean MSE (normalized data, 100 epochs):  36.81601981643332\n",
      "Standard deviation of MSE (normalized data, 100 epochs):  13.48632291937412\n",
      "\n",
      "D\n",
      "Mean MSE (normalized data, 3 hidden layers):  28.696288404419615\n",
      "Standard deviation of MSE (normalized data, 3 hidden layers):  12.984027119867813\n"
     ]
    }
   ],
   "source": [
    "print('\\nA')\n",
    "print('Mean MSE: ', mean_mse)\n",
    "print('Standard deviation of MSE: ', sd_mse)\n",
    "\n",
    "print('\\nB')\n",
    "print('Mean MSE (normalized data): ', mean_mse_norm)\n",
    "print('Standard deviation of MSE (normalized data): ', sd_mse_norm)\n",
    "\n",
    "print('\\nC')\n",
    "print('Mean MSE (normalized data, 100 epochs): ', mean_mse_norm_100)\n",
    "print('Standard deviation of MSE (normalized data, 100 epochs): ', sd_mse_norm_100)\n",
    "\n",
    "print('\\nD')\n",
    "print('Mean MSE (normalized data, 3 hidden layers): ', mean_mse_norm_ml)\n",
    "print('Standard deviation of MSE (normalized data, 3 hidden layers): ', sd_mse_norm_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Mean Squared Error')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.pointplot(x = ['Section A', 'Section B', 'Section C', 'Section D'], y = [mean_mse,mean_mse_norm,mean_mse_norm_100,mean_mse_norm_ml], color = '#6495ed').set_title('Mean Squared Error')\n",
    "plt.xlabel('Section')\n",
    "plt.ylabel('Mean Squared Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalization changes the scale of the features, which can sometimes distort the intrinsic relationships in the data or increase the standard deviation by a few outliers, as seen from comparison of Mean and Standard deviation of MSE from parts A and B.\n",
    "\n",
    "- Doubling the number of epochs had a significant positive effect on the accuracy of the model. We can see that the Mean MSE was 54.62 with 50 epochs (Part B) and Mean MSE with 100 epochs is 36.82! And the standard deviation of MSE with 100 epochs is almost 6 times smaller than with 50 epochs.\n",
    "\n",
    "- Implementing 3 hidden layers instead of 1 also has increased the accuracy of the model greatly, even better than the result in Part C where we doubled the number of epochs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
